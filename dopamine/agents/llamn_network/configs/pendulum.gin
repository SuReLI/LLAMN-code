# Hyperparameters for a simple Rainbow-style Pendulum agent. The hyperparameters
# chosen achieve reasonable performance.
import dopamine.agents.llamn_network.llamn_agent
import dopamine.discrete_domains.llamn_gym_lib
import dopamine.discrete_domains.llamn_game_lib
import dopamine.discrete_domains.run_experiment
import dopamine.replay_memory.prioritized_replay_buffer
import gin.tf.external_configurables

# Games
MasterRunner.games_names = [['Gym-Pendulum.1:11']]

# Runner
Runner.num_iterations = 50
Runner.training_steps = 2500
Runner.max_steps_per_episode = 500

LLAMNRunner.num_iterations = 20
LLAMNRunner.training_steps = 2500
LLAMNRunner.max_steps_per_episode = 500

# Common Expert / LLAMN
update_horizon = 1
feature_size = 512
num_atoms = 51
vmax = 10.
expert_init_option = 2
distributional_night = True

# Important
n_features = 20
GymPreprocessing.n_features = %n_features
GymPreprocessing.n_redundant = 10
GymPreprocessing.n_repeated = 3
AMNAgent.replay_scheme = 'prioritized'   # Night PER
AMNAgent.llamn_init_copy = True          # Copy weights of AMN between nights
AMNAgent.optimize_loss_sum = False       # Optimize losses independently or with sum
LLAMNRunner.buffer_prefill = None        # How to fill the night buffers (None, "copy" or "exploration")
LLAMNRunner.nb_steps_per_steps = -1      # How many steps to do alternatively on each game
                                         # Negative value means alternate episodes


observation_shape = (%n_features, )
observation_dtype = %llamn_game_lib.PENDULUM_OBSERVATION_DTYPE
stack_size = %llamn_game_lib.PENDULUM_STACK_SIZE


# Expert
ExpertAgent.observation_shape = %observation_shape
ExpertAgent.observation_dtype = %observation_dtype
ExpertAgent.stack_size = %stack_size
ExpertAgent.init_option = %expert_init_option
ExpertAgent.feature_size = %feature_size
ExpertAgent.network = @llamn_gym_lib.GymExpertNetwork
ExpertAgent.num_atoms = %num_atoms
ExpertAgent.vmax = %vmax
ExpertAgent.gamma = 0.99
ExpertAgent.update_horizon = %update_horizon
ExpertAgent.min_replay_history = 500
ExpertAgent.update_period = 4
ExpertAgent.target_update_period = 100
ExpertAgent.epsilon_fn = @dqn_agent.identity_epsilon
ExpertAgent.replay_scheme = 'uniform'
ExpertAgent.tf_device = '/gpu:0'  # use '/cpu:*' for non-GPU version
ExpertAgent.optimizer = @tf.train.AdamOptimizer()

tf.train.AdamOptimizer.learning_rate = 0.001
tf.train.AdamOptimizer.epsilon = 0.0003125

# AMN
AMNAgent.observation_shape = %observation_shape
AMNAgent.observation_dtype = %observation_dtype
AMNAgent.stack_size = %stack_size
AMNAgent.feature_size = %feature_size
AMNAgent.network = @llamn_gym_lib.GymAMNNetwork
AMNAgent.expert_network = @llamn_gym_lib.GymExpertNetwork
AMNAgent.distributional_night = %distributional_night
AMNAgent.expert_init_option = %expert_init_option
AMNAgent.expert_num_atoms = %num_atoms
AMNAgent.expert_vmax = %vmax
AMNAgent.min_replay_history = 500  # agent steps
AMNAgent.epsilon_fn = @dqn_agent.identity_epsilon
AMNAgent.update_horizon = %update_horizon
AMNAgent.tf_device = '/gpu:0'  # use '/cpu:*' for non-GPU version
AMNAgent.optimizer = @tf.train.AdamOptimizer()

# Replay Buffer
WrappedPrioritizedReplayBuffer.replay_capacity = 50000
WrappedPrioritizedReplayBuffer.batch_size = 128
